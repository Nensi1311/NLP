{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6030ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import math\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5ae6fa",
   "metadata": {},
   "source": [
    "## 1. N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdca24ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    # added padding with '#' characters to handle the start of sequences\n",
    "    padded_text = '#' * (n-1) + text\n",
    "    ngrams = []\n",
    "    for i in range(len(padded_text) - n + 1):\n",
    "        ngram = tuple(padded_text[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf877bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-Level Bigrams: [('#', 'h'), ('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o'), ('o', ' '), (' ', 'w'), ('w', 'o'), ('o', 'r'), ('r', 'l'), ('l', 'd')]\n"
     ]
    }
   ],
   "source": [
    "text = \"hello world\"\n",
    "\n",
    "# generate and display bigrams\n",
    "bigrams = generate_ngrams(text, 2)\n",
    "print(\"Character-Level Bigrams:\", bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ddb2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_model(corpus, n):\n",
    "    model = defaultdict(Counter)\n",
    "    ngrams = generate_ngrams(corpus, n)\n",
    "    \n",
    "    # build the model\n",
    "    for ngram in ngrams:\n",
    "        context = ngram[:-1]\n",
    "        char = ngram[-1]\n",
    "        model[context][char] += 1\n",
    "        \n",
    "    # convert counts to probabilities\n",
    "    for context in model:\n",
    "        total_count = sum(model[context].values())\n",
    "        for char in model[context]:\n",
    "            model[context][char] = model[context][char] / total_count\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d39e91",
   "metadata": {},
   "source": [
    "## 2. Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f80eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_smoothing(model, vocabulary_size, alpha=1.0):\n",
    "    smoothed_model = defaultdict(Counter)\n",
    "    for prefix, char_counts in model.items():\n",
    "        total_count = sum(char_counts.values()) + alpha * vocabulary_size\n",
    "        for char in char_counts:\n",
    "            smoothed_model[prefix][char] = (char_counts[char] + alpha) / total_count\n",
    "        for char in range(vocabulary_size):\n",
    "            if char not in char_counts:\n",
    "                smoothed_model[prefix][char] = alpha / total_count\n",
    "    return smoothed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42969ca0",
   "metadata": {},
   "source": [
    "## 3. Generating Text Using the N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b788b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, n, start_text, length=100):\n",
    "    current_text = list(start_text)\n",
    "    \n",
    "    # generate characters\n",
    "    for _ in range(length):\n",
    "        context = tuple(current_text[-(n-1):]) if len(current_text) >= n-1 else tuple('#' * (n-1 - len(current_text)) + ''.join(current_text))\n",
    "        \n",
    "        if context not in model:\n",
    "            break\n",
    "        \n",
    "        # get probability distribution for next character\n",
    "        char_dist = model[context]\n",
    "        \n",
    "        # sample next character\n",
    "        chars, probs = zip(*char_dist.items())\n",
    "        next_char = random.choices(chars, weights=probs)[0]\n",
    "        \n",
    "        # append to generated text\n",
    "        current_text.append(next_char)\n",
    "        \n",
    "    return ''.join(current_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2232f730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: hexthe thele\n"
     ]
    }
   ],
   "source": [
    "text = \"hello world this is a sample text for testing the n-gram model\"\n",
    "\n",
    "bigram_model = build_ngram_model(text, 2)\n",
    "\n",
    "generated = generate_text(bigram_model, 2, \"he\", 10)\n",
    "print(f\"Generated text: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb804f86",
   "metadata": {},
   "source": [
    "## 4. Evaluating the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10afce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, n, test_text):\n",
    "    ngrams = generate_ngrams(test_text, n)\n",
    "    log_prob = 0\n",
    "    total_ngrams = len(ngrams)\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        context = ngram[:-1]\n",
    "        char = ngram[-1]\n",
    "        \n",
    "        if context in model and char in model[context]:\n",
    "            prob = model[context][char]\n",
    "            log_prob += -1 * math.log2(prob)\n",
    "        else:\n",
    "            return float('inf')\n",
    "    return 2 ** (log_prob / total_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77a5545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. \n",
    "She sells seashells by the seashore. \n",
    "How much wood would a woodchuck chuck if a woodchuck could chuck wood? \n",
    "To be or not to be, that is the question. \n",
    "All that glitters is not gold. \n",
    "A journey of a thousand miles begins with a single step. \n",
    "Actions speak louder than words. \n",
    "Beauty is in the eye of the beholder. \n",
    "Every cloud has a silver lining. \n",
    "Fortune favors the bold and brave. \n",
    "Life is like a box of chocolates. \n",
    "The early bird catches the worm. \n",
    "Where there's smoke, there's fire. \n",
    "Time heals all wounds and teaches all things. \n",
    "Knowledge is power, and power corrupts. \n",
    "Practice makes perfect, but nobody's perfect. \n",
    "The pen is mightier than the sword. \n",
    "When in Rome, do as the Romans do. \n",
    "A picture is worth a thousand words. \n",
    "Better late than never, but never late is better.\n",
    "Experience is the best teacher of all things.\n",
    "Laughter is the best medicine for the soul.\n",
    "Music soothes the savage beast within us.\n",
    "Nothing ventured, nothing gained in life.\n",
    "The grass is always greener on the other side.\n",
    "\"\"\"\n",
    "\n",
    "# clean the corpus\n",
    "training_corpus = ''.join(c.lower() for c in training_corpus if c.isalnum() or c.isspace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "206334d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthe quick brown fox jumps over the lazy dog \\nshe sells seashells by the seashore \\nhow much wood would a woodchuck chuck if a woodchuck could chuck wood \\nto be or not to be that is the question \\nall that glitters is not gold \\na journey of a thousand miles begins with a single step \\nactions speak louder than words \\nbeauty is in the eye of the beholder \\nevery cloud has a silver lining \\nfortune favors the bold and brave \\nlife is like a box of chocolates \\nthe early bird catches the worm \\nwhere theres smoke theres fire \\ntime heals all wounds and teaches all things \\nknowledge is power and power corrupts \\npractice makes perfect but nobodys perfect \\nthe pen is mightier than the sword \\nwhen in rome do as the romans do \\na picture is worth a thousand words \\nbetter late than never but never late is better\\nexperience is the best teacher of all things\\nlaughter is the best medicine for the soul\\nmusic soothes the savage beast within us\\nnothing ventured nothing gained in life\\nthe grass is always greener on the other side\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80f7ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(corpus):\n",
    "    models = {}\n",
    "    for n in [2, 3, 4]:\n",
    "        models[n] = build_ngram_model(corpus, n)\n",
    "    return models\n",
    "\n",
    "models = build_models(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c0cf1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_samples(models, num_samples=10, sample_length=40):\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for n, model in models.items():\n",
    "        print(f\"\\n=== {n}-gram Model Evaluation ===\")\n",
    "        \n",
    "        start_text = training_corpus[:n-1]\n",
    "        for i in range(num_samples):\n",
    "            generated = generate_text(model, n, start_text, sample_length)\n",
    "            perplexity = calculate_perplexity(model, n, generated)\n",
    "            \n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(f\"Text: {generated}\")\n",
    "            print(f\"Perplexity: {perplexity:.2f}\")\n",
    "            \n",
    "            results[n].append({\n",
    "                'text': generated,\n",
    "                'perplexity': perplexity\n",
    "            })\n",
    "        \n",
    "        avg_perplexity = sum(sample['perplexity'] for sample in results[n]) / len(results[n])\n",
    "        print(f\"\\nAverage Perplexity for {n}-gram model: {avg_perplexity}:.2f\")\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d87a5d51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2-gram Model Evaluation ===\n",
      "\n",
      "Sample 1:\n",
      "Text: \n",
      "winicather s ince pioro p lwo thack fowo\n",
      "Perplexity: 8.28\n",
      "\n",
      "Sample 2:\n",
      "Text: \n",
      "ndy ioldchtox qud ld ir wha thethte insh\n",
      "Perplexity: 8.40\n",
      "\n",
      "Sample 3:\n",
      "Text: \n",
      "ectermisins atha ts hun ferneanotereeswh\n",
      "Perplexity: 9.67\n",
      "\n",
      "Sample 4:\n",
      "Text: \n",
      "bex the t thes tuinglore ownsagisen mir \n",
      "Perplexity: 7.62\n",
      "\n",
      "Sample 5:\n",
      "Text: \n",
      "be tofer s inglitthed \n",
      "nted thean \n",
      "whe t\n",
      "Perplexity: 6.25\n",
      "\n",
      "Sample 6:\n",
      "Text: \n",
      "f s boucthell\n",
      "ttud sisa s \n",
      "the\n",
      "tees \n",
      "azy\n",
      "Perplexity: 7.34\n",
      "\n",
      "Sample 7:\n",
      "Text: \n",
      "th s \n",
      "e werogllord s hexpthe \n",
      "sty theras\n",
      "Perplexity: 7.15\n",
      "\n",
      "Sample 8:\n",
      "Text: \n",
      "s tha dis ponghttinord \n",
      "muche thangr\n",
      "be \n",
      "Perplexity: 7.02\n",
      "\n",
      "Sample 9:\n",
      "Text: \n",
      "he prtell wous ber mathols agermins chay\n",
      "Perplexity: 7.87\n",
      "\n",
      "Sample 10:\n",
      "Text: \n",
      "t llox cke s n erd sindche l cte byellot\n",
      "Perplexity: 8.07\n",
      "\n",
      "Average Perplexity for 2-gram model: 7.7661008231753375:.2f\n",
      "\n",
      "=== 3-gram Model Evaluation ===\n",
      "\n",
      "Sample 1:\n",
      "Text: \n",
      "to \n",
      "shors peaughtions ney bodys ned \n",
      "a be\n",
      "Perplexity: 3.29\n",
      "\n",
      "Sample 2:\n",
      "Text: \n",
      "the beging ver lifect glithes sell thatea\n",
      "Perplexity: 2.36\n",
      "\n",
      "Sample 3:\n",
      "Text: \n",
      "to \n",
      "the \n",
      "exper of a sider clound woude\n",
      "th\n",
      "Perplexity: 2.48\n",
      "\n",
      "Sample 4:\n",
      "Text: \n",
      "ther\n",
      "exper to bire ques \n",
      "forrupts thorthe\n",
      "Perplexity: 2.49\n",
      "\n",
      "Sample 5:\n",
      "Text: \n",
      "ther of a sell the of and \n",
      "achuck likes a\n",
      "Perplexity: 2.21\n",
      "\n",
      "Sample 6:\n",
      "Text: \n",
      "tion usand \n",
      "to a pic sin fire rower lould\n",
      "Perplexity: 2.65\n",
      "\n",
      "Sample 7:\n",
      "Text: \n",
      "to bromak is speachuche lourner a pick is\n",
      "Perplexity: 2.94\n",
      "\n",
      "Sample 8:\n",
      "Text: \n",
      "time \n",
      "to a bromed an thin ner searly chuc\n",
      "Perplexity: 2.96\n",
      "\n",
      "Sample 9:\n",
      "Text: \n",
      "tiene \n",
      "pravage gractureen \n",
      "wheall the woo\n",
      "Perplexity: 2.90\n",
      "\n",
      "Sample 10:\n",
      "Text: \n",
      "thands gold words is likes always pow me \n",
      "Perplexity: 2.53\n",
      "\n",
      "Average Perplexity for 3-gram model: 2.6810606334537304:.2f\n",
      "\n",
      "=== 4-gram Model Evaluation ===\n",
      "\n",
      "Sample 1:\n",
      "Text: \n",
      "the late the rome heals beashore than neve\n",
      "Perplexity: 1.58\n",
      "\n",
      "Sample 2:\n",
      "Text: \n",
      "theres a bolder silver on \n",
      "all wounds \n",
      "tim\n",
      "Perplexity: 1.84\n",
      "\n",
      "Sample 3:\n",
      "Text: \n",
      "the quick wounds always greener side\n",
      "\n",
      "Perplexity: 1.49\n",
      "\n",
      "Sample 4:\n",
      "Text: \n",
      "theres the woul\n",
      "music sould \n",
      "all things\n",
      "la\n",
      "Perplexity: 1.63\n",
      "\n",
      "Sample 5:\n",
      "Text: \n",
      "the or the savage beauty is not to beast t\n",
      "Perplexity: 1.67\n",
      "\n",
      "Sample 6:\n",
      "Text: \n",
      "the be than words \n",
      "time do as the perience\n",
      "Perplexity: 1.60\n",
      "\n",
      "Sample 7:\n",
      "Text: \n",
      "the othe betters is like a would a thingle\n",
      "Perplexity: 1.87\n",
      "\n",
      "Sample 8:\n",
      "Text: \n",
      "the quick brave \n",
      "life is the seashe greene\n",
      "Perplexity: 1.50\n",
      "\n",
      "Sample 9:\n",
      "Text: \n",
      "the box of than the begins do as a bold \n",
      "t\n",
      "Perplexity: 1.74\n",
      "\n",
      "Sample 10:\n",
      "Text: \n",
      "thes thin romans speak loud has the grass \n",
      "Perplexity: 1.54\n",
      "\n",
      "Average Perplexity for 4-gram model: 1.6454772575091543:.2f\n",
      "\n",
      "=== Overall Statistics ===\n",
      "\n",
      "2-gram Model Statistics:\n",
      "Minimum Perplexity: 6.25\n",
      "Maximum Perplexity: 9.67\n",
      "Average Perplexity: 7.77\n",
      "\n",
      "3-gram Model Statistics:\n",
      "Minimum Perplexity: 2.21\n",
      "Maximum Perplexity: 3.29\n",
      "Average Perplexity: 2.68\n",
      "\n",
      "4-gram Model Statistics:\n",
      "Minimum Perplexity: 1.49\n",
      "Maximum Perplexity: 1.87\n",
      "Average Perplexity: 1.65\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_samples(models)\n",
    "\n",
    "print(\"\\n=== Overall Statistics ===\")\n",
    "for n in models.keys():\n",
    "    perplexities = [sample['perplexity'] for sample in results[n]]\n",
    "    min_prep = min(perplexities)\n",
    "    max_prep = max(perplexities)\n",
    "    avg_prep = sum(perplexities) / len(perplexities)\n",
    "    \n",
    "    print(f\"\\n{n}-gram Model Statistics:\")\n",
    "    print(f\"Minimum Perplexity: {min_prep:.2f}\")\n",
    "    print(f\"Maximum Perplexity: {max_prep:.2f}\")\n",
    "    print(f\"Average Perplexity: {avg_prep:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ddff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
